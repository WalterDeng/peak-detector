{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d76c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67689042",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3,2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49dbf05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6075, 0.9700],\n",
       "        [0.5445, 0.3502],\n",
       "        [0.8606, 0.8579]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75277da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "837c86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2ff149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac258ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(4,4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73defa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a,a,a], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e93640a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29c37ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand(4,4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5aa54f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9191, 0.6502, 0.7700, 0.8032],\n",
       "        [0.5854, 0.3410, 0.8978, 0.4458],\n",
       "        [0.4713, 0.4927, 0.6405, 0.0373],\n",
       "        [0.0433, 0.0675, 0.9581, 0.0094]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d822c5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9191, 0.6502, 0.7700, 0.8032],\n",
       "        [0.5854, 0.3410, 0.8978, 0.4458],\n",
       "        [0.4713, 0.4927, 0.6405, 0.0373],\n",
       "        [0.0433, 0.0675, 0.9581, 0.0094]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2109f136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8382, 1.3004, 1.5400, 1.6064],\n",
       "        [1.1707, 0.6821, 1.7955, 0.8915],\n",
       "        [0.9426, 0.9854, 1.2810, 0.0747],\n",
       "        [0.0865, 0.1351, 1.9162, 0.0188]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82b957c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8382, 1.3004, 1.5400, 1.6064],\n",
       "        [1.1707, 0.6821, 1.7955, 0.8915],\n",
       "        [0.9426, 0.9854, 1.2810, 0.0747],\n",
       "        [0.0865, 0.1351, 1.9162, 0.0188]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.mul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "337c6e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.2849, 6.2849, 6.2849, 6.2849],\n",
       "        [4.5399, 4.5399, 4.5399, 4.5399],\n",
       "        [3.2838, 3.2838, 3.2838, 3.2838],\n",
       "        [2.1566, 2.1566, 2.1566, 2.1566]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "922bba37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.2849, 6.2849, 6.2849, 6.2849],\n",
       "        [4.5399, 4.5399, 4.5399, 4.5399],\n",
       "        [3.2838, 3.2838, 3.2838, 3.2838],\n",
       "        [2.1566, 2.1566, 2.1566, 2.1566]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a95a1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 11., 11., 11.],\n",
       "        [11., 11., 11., 11.],\n",
       "        [11., 11., 11., 11.],\n",
       "        [11., 11., 11., 11.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add_(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ea109cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 11., 11., 11.],\n",
       "        [11., 11., 11., 11.],\n",
       "        [11., 11., 11., 11.],\n",
       "        [11., 11., 11., 11.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "986f0b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 16., 16., 16.],\n",
       "       [16., 16., 16., 16.],\n",
       "       [16., 16., 16., 16.],\n",
       "       [16., 16., 16., 16.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.numpy() + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94d85486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 11., 11., 11.],\n",
       "        [11., 11., 11., 11.],\n",
       "        [11., 11., 11., 11.],\n",
       "        [11., 11., 11., 11.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.from_numpy(a.numpy())\n",
    "a.add(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "137993a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38ed610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/yoshi/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1,3,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44cd8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5cbd4f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1766, -4.5227, -5.1190, -6.1572, -4.8472, -4.5900, -4.6345, -3.8145,\n",
       "         -4.1280, -5.1853, -5.5691, -5.8319, -5.0105, -5.5339, -5.7332, -5.0775,\n",
       "         -5.2158, -4.6883, -5.2339, -5.3225, -6.1369, -4.9384, -5.9312, -4.1538,\n",
       "         -5.4517, -5.4014, -5.1622, -5.4322, -5.2572, -4.9564, -5.3528, -5.0883,\n",
       "         -4.6122, -4.6986, -4.2108, -4.6456, -3.7954, -5.1984, -4.9455, -4.5413,\n",
       "         -5.3168, -5.3203, -5.4200, -4.8124, -5.0992, -4.6699, -5.1533, -5.0170,\n",
       "         -5.5528, -5.5345, -5.0476, -4.0797, -4.5405, -5.0087, -4.0777, -5.4470,\n",
       "         -4.5325, -5.9573, -5.0462, -4.6391, -3.5551, -4.0279, -4.4717, -4.1070,\n",
       "         -5.0845, -4.2682, -4.4786, -4.4734, -4.9753, -5.5780, -6.1981, -4.3016,\n",
       "         -5.6418, -4.4807, -5.4297, -5.6051, -4.2121, -5.0111, -4.2716, -4.4682,\n",
       "         -5.0540, -5.8600, -4.5787, -5.2224, -5.1310, -4.2534, -4.2902, -4.2835,\n",
       "         -4.4959, -5.3267, -5.7092, -5.6005, -6.4199, -4.9424, -4.5642, -6.5699,\n",
       "         -5.0500, -4.9058, -5.7814, -4.9377, -5.7898, -5.5323, -5.2134, -4.2148,\n",
       "         -4.4483, -4.9234, -4.8798, -5.6299, -5.4987, -5.9117, -5.2904, -5.1708,\n",
       "         -3.0916, -4.2621, -4.3774, -5.2577, -5.2222, -4.5035, -3.9226, -4.8663,\n",
       "         -5.3659, -4.4117, -4.4061, -4.5127, -3.6813, -4.4698, -4.2571, -6.2240,\n",
       "         -5.9171, -5.9107, -6.1246, -5.9815, -5.6248, -6.1592, -5.3395, -5.8045,\n",
       "         -5.6136, -5.5927, -5.8734, -6.0697, -6.0948, -6.1902, -6.5379, -6.1672,\n",
       "         -4.9287, -4.5784, -5.3359, -6.2655, -5.6570, -5.5933, -3.7983, -2.9913,\n",
       "         -5.3444, -5.0852, -4.2050, -4.3273, -4.7387, -4.7704, -4.4857, -4.6961,\n",
       "         -4.0948, -3.9355, -4.2888, -4.1483, -4.2953, -4.6664, -4.9430, -5.1140,\n",
       "         -3.9443, -4.7114, -4.6495, -3.8650, -4.2358, -4.4487, -4.3065, -5.1621,\n",
       "         -4.6120, -4.5283, -4.0064, -4.1645, -4.2207, -4.6395, -3.9128, -4.4809,\n",
       "         -3.6517, -3.8524, -3.9580, -4.3805, -4.3265, -4.0019, -4.8611, -3.9879,\n",
       "         -4.2298, -4.0454, -5.0607, -3.9287, -4.5938, -4.3632, -4.2580, -3.9380,\n",
       "         -4.5632, -4.3100, -3.7578, -3.8793, -4.6722, -4.3167, -4.5675, -4.0105,\n",
       "         -3.3919, -3.9081, -4.7652, -4.3699, -3.9367, -4.6106, -4.4663, -4.2745,\n",
       "         -4.6734, -4.2820, -4.9314, -4.0577, -4.3265, -4.4505, -4.4247, -3.9570,\n",
       "         -4.3244, -4.3060, -4.4272, -3.7727, -4.9468, -4.7237, -4.6407, -4.3186,\n",
       "         -4.3186, -4.5154, -3.7748, -3.7143, -3.9852, -4.1398, -3.8971, -4.5920,\n",
       "         -4.1076, -4.4735, -4.4371, -4.5479, -4.8368, -4.2551, -4.1917, -4.6203,\n",
       "         -3.9681, -4.4184, -4.0025, -4.0243, -5.3271, -4.1405, -3.7209, -5.0994,\n",
       "         -4.2130, -4.2913, -4.5919, -4.3672, -4.7461, -5.1972, -4.7284, -4.2915,\n",
       "         -3.7997, -3.8641, -4.4427, -3.9088, -4.5628, -5.0310, -5.4966, -5.6611,\n",
       "         -4.9945, -3.7913, -5.6579, -5.8473, -5.4960, -5.2892, -5.9211, -5.1073,\n",
       "         -4.8735, -3.7378, -3.8019, -4.4669, -4.0480, -3.4508, -4.6306, -4.5318,\n",
       "         -5.1132, -6.0360, -5.4054, -5.9272, -4.8475, -5.5040, -5.3803, -5.5738,\n",
       "         -5.2942, -5.8296, -4.8532, -4.3089, -6.5262, -5.3522, -5.0343, -5.0174,\n",
       "         -5.7592, -5.0662, -4.4237, -5.6151, -6.0831, -5.2422, -4.3162, -5.0301,\n",
       "         -4.9652, -4.4191, -3.6054, -5.0898, -5.4844, -5.7813, -5.9755, -5.1923,\n",
       "         -6.0948, -5.5785, -6.0416, -6.2063, -6.3006, -6.6553, -5.7680, -4.1761,\n",
       "         -4.7110, -5.0819, -4.1395, -4.6725, -4.4983, -4.2155, -5.1223, -5.4706,\n",
       "         -6.1706, -3.9996, -4.0319, -5.8212, -5.0863, -3.7450, -4.8244, -5.4362,\n",
       "         -5.0622, -3.8980, -5.3434, -6.0764, -4.6804, -5.7474, -5.7655, -6.8385,\n",
       "         -5.9496, -5.4790, -5.2457, -4.0244, -3.2044, -4.3694, -4.1044, -4.0611,\n",
       "         -4.5537, -4.2695, -4.5564, -4.1963, -4.9515, -5.0158, -5.4170, -4.9682,\n",
       "         -5.2847, -5.1420, -5.1684, -4.5819, -4.7444, -4.5656, -4.5989, -5.5386,\n",
       "         -5.7903, -4.4782, -4.9771, -5.1879, -4.2071, -4.8804, -4.7995, -4.8956,\n",
       "         -5.1111, -4.7396, -5.5620, -5.6950, -5.6235, -4.6047, -3.4591, -4.2105,\n",
       "         -5.7145, -6.0022, -4.5828, -3.9846, -5.3724, -4.9885, -4.2350, -4.1821,\n",
       "         -5.1011, -3.7451, -4.1720, -6.7457, -5.9187, -4.8174, -4.6863, -4.7666,\n",
       "         -4.6942, -3.3814, -4.7972, -4.3441, -2.2807, -3.2044, -4.1610, -3.7010,\n",
       "         -4.6120, -4.1089, -4.1247, -3.3265, -3.7720, -3.0583, -4.3915, -4.3193,\n",
       "         -4.4100, -5.3262, -4.6403, -3.1846, -2.6438, -4.0335, -5.0977, -4.6053,\n",
       "         -4.0534, -3.8521, -4.0906, -3.7711, -4.6319, -4.9535, -4.3137, -4.3916,\n",
       "         -3.3292, -3.6185, -4.3993, -5.1856, -5.0157, -4.2961, -4.1488, -2.9391,\n",
       "         -3.7201, -4.9176, -4.4163, -4.1498, -4.1168, -4.6899, -4.7746, -4.0406,\n",
       "         -2.7638, -3.0671, -4.8445, -3.8145, -5.0300, -4.0526, -3.3158, -2.0662,\n",
       "         -3.8043, -4.8164, -5.6277, -4.4569, -4.7101, -2.7873, -3.6669, -3.9827,\n",
       "         -4.0747, -3.5587, -4.9732, -4.5359, -4.4849, -4.2558, -3.7122, -4.1473,\n",
       "         -4.3781, -3.8825, -3.8651, -5.4324, -5.7439, -4.2536, -4.8219, -3.0758,\n",
       "         -3.1067, -4.0374, -4.3405, -3.7717, -3.7188, -5.7211, -3.2455, -5.6066,\n",
       "         -4.5226, -4.8038, -4.6644, -2.9871, -5.9393, -3.7689, -3.4140, -3.7128,\n",
       "         -3.4053, -3.0809, -3.4437, -4.0828, -4.1655, -4.2551, -5.6046, -5.3831,\n",
       "         -3.9163, -4.1415, -3.4286, -2.6597, -4.0271, -4.6897, -3.3134, -3.8036,\n",
       "         -5.4203, -4.0364, -3.9731, -2.8435, -4.4197, -4.9723, -4.7658, -4.9450,\n",
       "         -4.0998, -4.5084, -3.3167, -4.1125, -4.5955, -5.4328, -4.0652, -5.0787,\n",
       "         -4.9748, -5.2305, -4.5858, -3.2699, -6.0661, -2.7719, -3.4193, -3.4338,\n",
       "         -3.9124, -3.2710, -3.7063, -6.9239, -5.6786, -4.6890, -4.8796, -4.7335,\n",
       "         -3.8715, -4.4817, -6.6877, -5.6091, -4.2084, -4.1822, -2.8733, -3.6781,\n",
       "         -4.5441, -4.7211, -3.6984, -4.3804, -5.6920, -5.2242, -4.0014, -3.3313,\n",
       "         -4.2370, -4.9954, -3.3141, -4.3097, -3.4580, -5.2341, -4.0288, -4.7438,\n",
       "         -5.7035, -3.1217, -4.4065, -3.9508, -4.3481, -4.9217, -3.5074, -4.1431,\n",
       "         -3.4706, -3.6072, -5.0428, -2.7861, -3.2530, -2.7926, -4.8850, -3.9051,\n",
       "         -5.0462, -3.6199, -3.8901, -5.4036, -3.3312, -4.6582, -4.7599, -3.6066,\n",
       "         -2.3413, -4.6515, -4.5213, -5.1756, -4.2227, -4.1187, -2.9316, -4.9729,\n",
       "         -3.8674, -4.8065, -3.5436, -3.6949, -5.2867, -3.7483, -4.4271, -4.1459,\n",
       "         -3.5094, -3.7897, -2.4980, -3.5384, -3.0244, -4.1794, -4.1106, -4.0417,\n",
       "         -4.2887, -5.5651, -3.3826, -4.6343, -5.6362, -4.2831, -4.2607, -3.5176,\n",
       "         -3.6723, -3.4003, -4.3548, -4.5041, -3.3059, -3.8765, -3.6822, -4.1848,\n",
       "         -6.5016, -3.2743, -4.4722, -2.9335, -3.6298, -5.3648, -3.5494, -4.1564,\n",
       "         -4.9243, -5.8099, -3.2549, -4.0922, -3.8450, -3.6205, -4.5564, -4.0723,\n",
       "         -4.5241, -4.3361, -4.7276, -3.8398, -4.5221, -5.8450, -4.4980, -5.1818,\n",
       "         -3.3578, -4.6698, -3.1820, -4.1573, -5.6078, -4.8416, -4.3679, -4.8303,\n",
       "         -4.6888, -3.6939, -2.8921, -5.0078, -3.1004, -3.5547, -3.7765, -4.4895,\n",
       "         -3.6842, -3.6517, -5.0458, -3.8912, -3.4129, -5.9631, -4.7514, -5.5326,\n",
       "         -4.7665, -5.4244, -5.3295, -3.7292, -3.8081, -3.5782, -5.4177, -4.0506,\n",
       "         -2.9200, -4.5965, -5.1038, -3.7709, -2.6988, -4.8420, -4.5536, -4.0774,\n",
       "         -3.7810, -4.9285, -4.8436, -4.0895, -3.5906, -4.4448, -3.3902, -3.4758,\n",
       "         -4.5742, -4.7356, -3.8069, -4.8840, -4.0587, -5.1375, -4.9554, -3.6791,\n",
       "         -4.2170, -4.5242, -2.8509, -4.3969, -5.2960, -3.0334, -5.0543, -4.7537,\n",
       "         -2.9040, -5.0777, -4.5812, -2.1248, -4.8429, -2.5709, -6.0547, -4.5695,\n",
       "         -4.3046, -3.7461, -3.6278, -4.2371, -3.1187, -4.7060, -3.7220, -4.3011,\n",
       "         -3.6968, -4.0270, -4.4618, -3.4740, -4.2147, -3.2028, -4.0545, -4.4794,\n",
       "         -4.2975, -3.7361, -3.4618, -4.8207, -3.5148, -4.7073, -3.1638, -4.6417,\n",
       "         -4.2554, -3.4031, -4.3142, -3.5877, -2.9489, -3.5624, -3.9971, -3.9648,\n",
       "         -4.6859, -3.0266, -4.3186, -4.0753, -3.0470, -3.8266, -3.9084, -4.0362,\n",
       "         -3.9988, -3.9563, -3.2235, -5.4599, -5.5500, -5.5530, -3.3609, -3.6964,\n",
       "         -2.9595, -4.5207, -4.2434, -3.2901, -4.2771, -4.4468, -4.0656, -3.4355,\n",
       "         -2.7082, -3.2383, -3.9731, -4.2590, -3.3866, -3.8509, -5.1710, -3.8698,\n",
       "         -5.2057, -3.7047, -5.2729, -5.8766, -3.0201, -3.0173, -4.3006, -3.7543,\n",
       "         -2.8838, -4.2505, -4.8841, -2.9516, -4.7015, -2.6896, -5.2829, -4.6183,\n",
       "         -4.1481, -5.5106, -2.7210, -3.9943, -6.1721, -5.5893, -4.0229, -3.6821,\n",
       "         -3.7107, -5.6433, -4.0271, -3.5288, -3.2253, -5.2911, -3.1798, -4.2079,\n",
       "         -5.2215, -5.4303, -4.0090, -3.7898, -2.6866, -2.7774, -3.3296, -5.1569,\n",
       "         -3.1383, -3.8524, -4.1843, -4.0034, -3.7215, -2.8221, -3.5476, -4.9323,\n",
       "         -3.7467, -3.4730, -3.5313, -3.1168, -2.9050, -5.1440, -4.6592, -3.7537,\n",
       "         -5.2383, -4.8901, -4.9746, -3.3354, -4.2454, -2.6809, -3.2398, -4.6633,\n",
       "         -4.9080, -3.9388, -4.4829, -4.6532, -2.9678, -4.8573, -3.8517, -6.0131,\n",
       "         -3.1612, -5.6832, -7.2055, -4.0724, -3.1289, -4.4496, -4.7490, -2.8262,\n",
       "         -3.5167, -4.6134, -3.2384, -3.0488, -4.4026, -4.2594, -4.7210, -4.7779,\n",
       "         -5.7474, -3.9756, -4.3208, -4.0577, -3.6455, -4.3951, -5.1999, -4.9662,\n",
       "         -3.4405, -3.6582, -2.3790, -2.2352, -5.9116, -4.6723, -2.9975, -3.4504,\n",
       "         -3.6016, -4.4274, -4.7134, -2.8291, -5.3244, -3.1547, -3.3027, -3.3633,\n",
       "         -3.8279, -4.6677, -6.1691, -5.1827, -4.0219, -3.9271, -3.9639, -4.4624,\n",
       "         -5.0269, -3.3596, -5.2980, -3.9074, -5.3305, -5.6217, -5.3642, -4.7232,\n",
       "         -4.3671, -3.1082, -4.7788, -4.5483, -4.0114, -6.5942, -4.5791, -5.1421,\n",
       "         -4.0483, -4.2074, -4.1560, -4.5730, -4.7620, -5.0146, -4.8389, -4.5456,\n",
       "         -5.0044, -5.2540, -5.6615, -4.2869, -4.0692, -5.0313, -4.3720, -5.3499,\n",
       "         -5.2092, -4.6408, -3.5431, -5.2050, -4.9546, -5.0259, -4.5609, -5.4354,\n",
       "         -3.9851, -4.1377, -5.1525, -5.1301, -5.6298, -4.9375, -3.7478, -4.9568,\n",
       "         -3.5069, -4.3741, -4.5142, -3.6547, -4.7974, -4.5225, -6.0837, -3.4347,\n",
       "         -6.0307, -3.8341, -4.2312, -5.1427, -5.1034, -4.2371, -4.1039, -4.7001,\n",
       "         -5.6497, -6.1116, -6.9006, -2.9716, -5.0648, -5.5423, -5.1413, -5.8195,\n",
       "         -5.4471, -6.5672, -5.1222, -4.7186, -4.3285, -4.9814, -3.1155, -3.3066]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(data)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6b8450e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-525.5311, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4a253114",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1941ffce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "baaf3e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<SubBackward0 at 0x7ff7384e7af0>, 0),)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86d96d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5658196",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c919abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.,3.], requires_grad=True)\n",
    "b = torch.tensor([6.,4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3205fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3 * a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "034d668b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m external_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.\u001b[39m,\u001b[38;5;241m1.\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1acb1989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 81.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "27caf9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 81.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9*a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d154c57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.,  -8.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "361c388d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 4.], requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5e69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
